#!/usr/bin/env python3
"""
HyperXray Log Collector ve Rapor Olu≈üturucu

Bu script:
1. ADB ile cihazdan loglarƒ± toplar
2. Log dosyalarƒ±nƒ± analiz eder
3. Detaylƒ± bir rapor olu≈üturur
"""

import subprocess
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any, Optional
import re

# Package name
PACKAGE_NAME = "com.hyperxray.an"

# Log dosya yollarƒ± (cihaz i√ßi)
DEVICE_LOG_PATHS = {
    "app_log": f"/data/data/{PACKAGE_NAME}/files/app_log.txt",
    "learner_log": f"/data/data/{PACKAGE_NAME}/files/learner_log.jsonl",
    "runtime_log": f"/data/data/{PACKAGE_NAME}/files/logs/tls_v5_runtime_log.jsonl",
    "dns_cache": f"/data/data/{PACKAGE_NAME}/cache/dns_cache.json",
    "stat_file": f"/data/data/{PACKAGE_NAME}/files/stat.txt",
    "logcat": None,  # Will be collected separately
}

# √áƒ±ktƒ± dizini
OUTPUT_DIR = Path("logs_collected")
OUTPUT_DIR.mkdir(exist_ok=True)

# Rapor dosyasƒ±
REPORT_FILE = OUTPUT_DIR / \
    f"log_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"


def run_adb_command(command: List[str]) -> Optional[str]:
    """ADB komutu √ßalƒ±≈ütƒ±r ve sonucu d√∂nd√ºr."""
    try:
        result = subprocess.run(
            ["adb"] + command,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
            check=False
        )
        if result.returncode == 0:
            return result.stdout.strip() if result.stdout else None
        else:
            error_msg = result.stderr.strip() if result.stderr else "Bilinmeyen hata"
            print(f"‚ö†Ô∏è  ADB komutu ba≈üarƒ±sƒ±z: {' '.join(command)}")
            if error_msg:
                print(f"   Hata: {error_msg}")
            return None
    except FileNotFoundError:
        print("‚ùå ADB bulunamadƒ±! L√ºtfen Android SDK'yƒ± y√ºkleyin.")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå ADB komutu hatasƒ±: {e}")
        return None


def check_adb_connection() -> bool:
    """ADB baƒülantƒ±sƒ±nƒ± kontrol et."""
    result = run_adb_command(["devices"])
    if result:
        lines = result.split("\n")[1:]  # ƒ∞lk satƒ±rƒ± atla
        devices = [line for line in lines if line.strip() and "device" in line]
        if devices:
            print(f"‚úÖ ADB baƒülantƒ±sƒ± ba≈üarƒ±lƒ±: {len(devices)} cihaz bulundu")
            return True
        else:
            print("‚ùå Baƒülƒ± cihaz bulunamadƒ±!")
            return False
    return False


def pull_log_file(device_path: str, local_name: str, check_exists: bool = True) -> Optional[Path]:
    """Cihazdan log dosyasƒ±nƒ± √ßek."""
    local_path = OUTPUT_DIR / local_name

    # √ñnce dosyanƒ±n var olup olmadƒ±ƒüƒ±nƒ± kontrol et (opsiyonel)
    if check_exists:
        result = run_adb_command(
            ["shell", "run-as", PACKAGE_NAME, "test", "-f", device_path])
        if result is None:
            print(f"‚ö†Ô∏è  Log dosyasƒ± bulunamadƒ±: {device_path}")
            return None

    # Dosyayƒ± √ßek (relative path kullan)
    # device_path'ten sadece dosya adƒ±nƒ± al
    relative_path = device_path.replace(f"/data/data/{PACKAGE_NAME}/", "")
    result = run_adb_command(
        ["shell", "run-as", PACKAGE_NAME, "cat", relative_path])
    if result is not None:
        local_path.write_text(result, encoding="utf-8", errors="ignore")
        size = local_path.stat().st_size
        print(f"‚úÖ {local_name} √ßekildi ({size:,} bytes)")
        return local_path
    else:
        print(f"‚ö†Ô∏è  {local_name} √ßekilemedi")
        return None


def collect_logcat(tag_filters: List[str] = None, lines: int = 100000) -> Optional[Path]:
    """Logcat'ten loglarƒ± detaylƒ± topla (artƒ±rƒ±lmƒ±≈ü satƒ±r limiti)."""
    local_path = OUTPUT_DIR / "logcat.txt"

    # √ñnce t√ºm loglarƒ± √ßek (filtreleme sonra yapƒ±labilir)
    # -d: dump and exit, -v time: timestamp ekle
    command = ["logcat", "-d", "-v", "time"]

    # Son N satƒ±rƒ± al (artƒ±rƒ±ldƒ±: 100000)
    command.extend(["-t", str(lines)])

    result = run_adb_command(command)
    if result:
        # Package ile ilgili loglarƒ± filtrele (DNS cache loglarƒ±nƒ± da dahil et)
        filtered_lines = []
        dns_tags = ["SystemDnsCacheServer", "DnsCacheManager", "DNS", "dns", "DnsUpstreamClient", "DnsSocketPool"]
        relevant_tags = [PACKAGE_NAME, "HyperXray", "TProxyService", "XrayRuntime", "TProxy", "Xray"]
        
        for line in result.split("\n"):
            if (any(tag in line for tag in relevant_tags) or 
                any(tag in line for tag in dns_tags)):
                filtered_lines.append(line)

        if filtered_lines:
            content = "\n".join(filtered_lines)
            local_path.write_text(content, encoding="utf-8", errors="ignore")
            size = local_path.stat().st_size
            print(
                f"‚úÖ Logcat √ßekildi ({size:,} bytes, {len(filtered_lines)} satƒ±r filtrelendi)")
            return local_path
        else:
            print("‚ö†Ô∏è  Logcat'te package ile ilgili log bulunamadƒ±")
            return None
    else:
        print("‚ö†Ô∏è  Logcat √ßekilemedi")
        return None


def analyze_app_log(log_file: Path) -> Dict[str, Any]:
    """Ana uygulama logunu analiz et."""
    if not log_file or not log_file.exists():
        return {"status": "not_found"}

    content = log_file.read_text(encoding="utf-8", errors="ignore")
    lines = content.split("\n")

    # Log seviyelerini say
    level_counts = Counter()
    tag_counts = Counter()
    error_patterns = []

    # Timestamp pattern
    timestamp_pattern = re.compile(r"(\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2})")

    for line in lines:
        if not line.strip():
            continue

        # Log seviyesi tespit et
        if "ERROR" in line.upper() or "[E]" in line:
            level_counts["ERROR"] += 1
        elif "WARN" in line.upper() or "[W]" in line:
            level_counts["WARN"] += 1
        elif "INFO" in line.upper() or "[I]" in line:
            level_counts["INFO"] += 1
        elif "DEBUG" in line.upper() or "[D]" in line:
            level_counts["DEBUG"] += 1

        # Tag tespit et
        tag_match = re.search(r"\[([^\]]+)\]", line)
        if tag_match:
            tag = tag_match.group(1)
            tag_counts[tag] += 1

        # Hata mesajlarƒ±nƒ± topla
        if "ERROR" in line.upper() or "Exception" in line or "Crash" in line:
            error_patterns.append(line[:200])  # ƒ∞lk 200 karakter

    # Timestamp aralƒ±ƒüƒ±nƒ± bul
    timestamps = timestamp_pattern.findall(content)
    time_range = None
    if timestamps:
        time_range = {
            "first": timestamps[0],
            "last": timestamps[-1],
            "count": len(timestamps)
        }

    return {
        "status": "analyzed",
        "total_lines": len(lines),
        "level_counts": dict(level_counts),
        "top_tags": dict(tag_counts.most_common(10)),
        "error_count": level_counts["ERROR"],
        "sample_errors": error_patterns[:10],
        "time_range": time_range
    }


def analyze_jsonl_log(log_file: Path) -> Dict[str, Any]:
    """JSONL formatƒ±ndaki loglarƒ± analiz et."""
    if not log_file or not log_file.exists():
        return {"status": "not_found"}

    entries = []
    errors = []

    for line_num, line in enumerate(log_file.read_text(encoding="utf-8", errors="ignore").split("\n"), 1):
        if not line.strip():
            continue

        try:
            entry = json.loads(line)
            entries.append(entry)
        except json.JSONDecodeError as e:
            errors.append(f"Satƒ±r {line_num}: {str(e)}")

    if not entries:
        return {"status": "empty"}

    # ƒ∞statistikler
    stats = {
        "total_entries": len(entries),
        "parse_errors": len(errors),
        "sample_errors": errors[:5]
    }

    # Learner log analizi
    if "svcClass" in entries[0]:
        # Learner log
        route_decisions = Counter(e.get("routeDecision", -1) for e in entries)
        success_rate = sum(1 for e in entries if e.get(
            "success", False)) / len(entries) * 100
        avg_latency = sum(e.get("latencyMs", 0)
                          for e in entries) / len(entries)
        avg_throughput = sum(e.get("throughputKbps", 0)
                             for e in entries) / len(entries)

        stats.update({
            "type": "learner",
            "route_decisions": dict(route_decisions),
            "success_rate": round(success_rate, 2),
            "avg_latency_ms": round(avg_latency, 2),
            "avg_throughput_kbps": round(avg_throughput, 2)
        })

    # Runtime log analizi
    elif "routingDecision" in entries[0]:
        # Runtime log
        routing_decisions = Counter(
            e.get("routingDecision", -1) for e in entries)
        success_rate = sum(1 for e in entries if e.get(
            "success", False)) / len(entries) * 100
        avg_latency = sum(e.get("latencyMs", 0)
                          for e in entries) / len(entries)
        avg_throughput = sum(e.get("throughputKbps", 0)
                             for e in entries) / len(entries)

        stats.update({
            "type": "runtime",
            "routing_decisions": dict(routing_decisions),
            "success_rate": round(success_rate, 2),
            "avg_latency_ms": round(avg_latency, 2),
            "avg_throughput_kbps": round(avg_throughput, 2)
        })

    return {
        "status": "analyzed",
        **stats
    }


def analyze_dns_cache(cache_file: Path) -> Dict[str, Any]:
    """DNS cache dosyasƒ±nƒ± detaylƒ± analiz et."""
    if not cache_file or not cache_file.exists():
        return {"status": "not_found"}

    try:
        content = cache_file.read_text(encoding="utf-8", errors="ignore")
        cache_data = json.loads(content)

        # Cache istatistikleri
        entries = cache_data.get("entries", {})
        total_entries = len(entries)
        current_time = int(datetime.now().timestamp())

        # TTL analizi
        ttls = []
        expired_entries = 0
        valid_entries = 0
        entry_ages = []
        ip_counts = []
        domain_categories = {
            "popular": 0,
            "cdn": 0,
            "api": 0,
            "other": 0
        }

        for domain, entry_data in entries.items():
            if isinstance(entry_data, dict):
                ttl = entry_data.get("ttl", 0)
                timestamp = entry_data.get("timestamp", 0)
                ips = entry_data.get("ips", [])
                
                if ttl > 0:
                    ttls.append(ttl)
                
                # Check if expired
                age = current_time - timestamp
                entry_ages.append(age)
                if age > ttl:
                    expired_entries += 1
                else:
                    valid_entries += 1
                
                # IP count
                if isinstance(ips, list):
                    ip_counts.append(len(ips))
                
                # Domain categorization
                domain_lower = domain.lower()
                if any(p in domain_lower for p in ["google", "facebook", "youtube", "twitter", "instagram", "amazon", "microsoft", "apple"]):
                    domain_categories["popular"] += 1
                elif any(cdn in domain_lower for cdn in [".cdn.", ".edge.", "cloudfront", "akamaiedge", "fastly"]):
                    domain_categories["cdn"] += 1
                elif any(api in domain_lower for api in ["api.", ".api", "api-"]):
                    domain_categories["api"] += 1
                else:
                    domain_categories["other"] += 1

        avg_ttl = sum(ttls) / len(ttls) if ttls else 0
        avg_age = sum(entry_ages) / len(entry_ages) if entry_ages else 0
        avg_ips = sum(ip_counts) / len(ip_counts) if ip_counts else 0
        min_ttl = min(ttls) if ttls else 0
        max_ttl = max(ttls) if ttls else 0

        # Domain analizi
        domains = list(entries.keys())
        popular_domains = [d for d in domains if any(
            p in d.lower() for p in ["google", "facebook", "youtube", "twitter", "instagram"])]

        # TTL distribution
        ttl_distribution = Counter()
        for ttl in ttls:
            if ttl < 3600:  # < 1 hour
                ttl_distribution["<1h"] += 1
            elif ttl < 86400:  # < 24 hours
                ttl_distribution["1h-24h"] += 1
            elif ttl < 172800:  # < 48 hours
                ttl_distribution["24h-48h"] += 1
            else:
                ttl_distribution[">48h"] += 1

        return {
            "status": "analyzed",
            "total_entries": total_entries,
            "valid_entries": valid_entries,
            "expired_entries": expired_entries,
            "expiry_rate": round((expired_entries / total_entries * 100) if total_entries > 0 else 0, 2),
            "avg_ttl_seconds": round(avg_ttl, 2),
            "min_ttl_seconds": min_ttl,
            "max_ttl_seconds": max_ttl,
            "avg_age_seconds": round(avg_age, 2),
            "avg_ips_per_entry": round(avg_ips, 2),
            "ttl_distribution": dict(ttl_distribution),
            "domain_categories": domain_categories,
            "popular_domains_count": len(popular_domains),
            "sample_domains": domains[:30],
            "cache_size_bytes": len(content.encode('utf-8'))
        }
    except json.JSONDecodeError as e:
        return {"status": "invalid_json", "error": str(e)}
    except Exception as e:
        return {"status": "error", "error": str(e)}


def analyze_dns_logs(app_log_file: Path, logcat_file: Path) -> Dict[str, Any]:
    """DNS loglarƒ±nƒ± detaylƒ± analiz et (app_log ve logcat'ten filtrele)."""
    dns_logs = []
    dns_cache_hits = 0
    dns_cache_misses = 0
    dns_resolved = 0
    dns_errors = 0
    dns_unhealthy_servers = 0
    dns_invalid_data = 0
    domains_queried = Counter()
    latency_times = []
    ttl_values = []
    dns_operations = Counter()  # initialize, save, get, cleanup, etc.
    dns_metrics_updates = 0
    dns_server_starts = 0
    dns_server_stops = 0

    # DNS ile ilgili tag'ler (geni≈ületilmi≈ü)
    dns_tags = ["SystemDnsCacheServer", "DnsCacheManager", "DnsUpstreamClient", 
                "DnsSocketPool", "DNS", "dns", "DnsCache"]

    # App log'dan DNS loglarƒ±nƒ± filtrele
    if app_log_file and app_log_file.exists():
        content = app_log_file.read_text(encoding="utf-8", errors="ignore")
        for line in content.split("\n"):
            if any(tag in line for tag in dns_tags):
                dns_logs.append(line)

                # Cache hit/miss sayƒ±sƒ±
                line_upper = line.upper()
                if "CACHE HIT" in line_upper or "cache hit" in line.lower():
                    dns_cache_hits += 1
                elif "CACHE MISS" in line_upper or "cache miss" in line.lower():
                    dns_cache_misses += 1
                elif "DNS resolved" in line_upper or "‚úÖ DNS" in line:
                    dns_resolved += 1

                # Domain √ßƒ±kar
                domain_match = re.search(
                    r'([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}', line)
                if domain_match:
                    domain = domain_match.group(0)
                    domains_queried[domain] += 1

                # Latency extraction
                latency_match = re.search(r'latency[:\s]+([\d.]+)\s*ms', line, re.IGNORECASE)
                if latency_match:
                    try:
                        latency_times.append(float(latency_match.group(1)))
                    except ValueError:
                        pass
                
                # TTL extraction
                ttl_match = re.search(r'TTL[:\s]+(\d+)', line, re.IGNORECASE)
                if ttl_match:
                    try:
                        ttl_values.append(int(ttl_match.group(1)))
                    except ValueError:
                        pass
                
                # DNS operations tracking
                if "initialized" in line.lower() or "initialize" in line.lower():
                    dns_operations["initialize"] += 1
                elif "save" in line.lower() and "cache" in line.lower():
                    dns_operations["save"] += 1
                elif "get" in line.lower() and "cache" in line.lower():
                    dns_operations["get"] += 1
                elif "cleanup" in line.lower() or "cleaned up" in line.lower():
                    dns_operations["cleanup"] += 1
                elif "metrics" in line.lower() and "update" in line.lower():
                    dns_metrics_updates += 1
                elif "started" in line.lower() and "server" in line.lower():
                    dns_server_starts += 1
                elif "stopped" in line.lower() and "server" in line.lower():
                    dns_server_stops += 1
                
                # Hata kontrol√º
                if "ERROR" in line_upper or "FAILED" in line_upper or "EXCEPTION" in line_upper:
                    dns_errors += 1
                elif "unhealthy" in line.lower():
                    dns_unhealthy_servers += 1
                elif "Invalid dataLength" in line or "invalid" in line.lower() and "dns" in line.lower():
                    dns_invalid_data += 1

    # Logcat'ten DNS loglarƒ±nƒ± filtrele
    if logcat_file and logcat_file.exists():
        content = logcat_file.read_text(encoding="utf-8", errors="ignore")
        for line in content.split("\n"):
            if any(tag in line for tag in dns_tags):
                dns_logs.append(line)

                line_upper = line.upper()
                if "CACHE HIT" in line_upper or "‚úÖ DNS CACHE HIT" in line:
                    dns_cache_hits += 1
                elif "CACHE MISS" in line_upper or "‚ö†Ô∏è DNS CACHE MISS" in line:
                    dns_cache_misses += 1
                elif "DNS resolved" in line_upper or "‚úÖ DNS resolved" in line:
                    dns_resolved += 1

                domain_match = re.search(
                    r'([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}', line)
                if domain_match:
                    domain = domain_match.group(0)
                    domains_queried[domain] += 1

                # Latency extraction (logcat)
                latency_match = re.search(r'latency[:\s]+([\d.]+)\s*ms', line, re.IGNORECASE)
                if latency_match:
                    try:
                        latency_times.append(float(latency_match.group(1)))
                    except ValueError:
                        pass
                
                # TTL extraction (logcat)
                ttl_match = re.search(r'TTL[:\s]+(\d+)', line, re.IGNORECASE)
                if ttl_match:
                    try:
                        ttl_values.append(int(ttl_match.group(1)))
                    except ValueError:
                        pass
                
                # DNS operations tracking (logcat)
                if "initialized" in line.lower() or "initialize" in line.lower():
                    dns_operations["initialize"] += 1
                elif "save" in line.lower() and "cache" in line.lower():
                    dns_operations["save"] += 1
                elif "get" in line.lower() and "cache" in line.lower():
                    dns_operations["get"] += 1
                elif "cleanup" in line.lower() or "cleaned up" in line.lower():
                    dns_operations["cleanup"] += 1
                elif "metrics" in line.lower() and "update" in line.lower():
                    dns_metrics_updates += 1
                elif "started" in line.lower() and "server" in line.lower():
                    dns_server_starts += 1
                elif "stopped" in line.lower() and "server" in line.lower():
                    dns_server_stops += 1
                
                if "ERROR" in line_upper or "FATAL" in line_upper:
                    dns_errors += 1
                elif "unhealthy" in line.lower():
                    dns_unhealthy_servers += 1
                elif "Invalid dataLength" in line:
                    dns_invalid_data += 1

    if not dns_logs:
        return {"status": "not_found"}

    total_queries = dns_cache_hits + dns_cache_misses + dns_resolved
    cache_hit_rate = (dns_cache_hits / total_queries *
                      100) if total_queries > 0 else 0
    
    # Calculate latency statistics
    avg_latency = sum(latency_times) / len(latency_times) if latency_times else 0
    min_latency = min(latency_times) if latency_times else 0
    max_latency = max(latency_times) if latency_times else 0
    
    # Calculate TTL statistics
    avg_ttl = sum(ttl_values) / len(ttl_values) if ttl_values else 0
    min_ttl = min(ttl_values) if ttl_values else 0
    max_ttl = max(ttl_values) if ttl_values else 0

    return {
        "status": "analyzed",
        "total_dns_logs": len(dns_logs),
        "cache_hits": dns_cache_hits,
        "cache_misses": dns_cache_misses,
        "dns_resolved": dns_resolved,
        "cache_hit_rate": round(cache_hit_rate, 2),
        "errors": dns_errors,
        "unhealthy_servers": dns_unhealthy_servers,
        "invalid_data_errors": dns_invalid_data,
        "unique_domains": len(domains_queried),
        "top_domains": dict(domains_queried.most_common(30)),
        "latency_stats": {
            "avg_ms": round(avg_latency, 2),
            "min_ms": round(min_latency, 2),
            "max_ms": round(max_latency, 2),
            "samples": len(latency_times)
        },
        "ttl_stats": {
            "avg_seconds": round(avg_ttl, 2),
            "min_seconds": min_ttl,
            "max_seconds": max_ttl,
            "samples": len(ttl_values)
        },
        "operations": dict(dns_operations),
        "metrics_updates": dns_metrics_updates,
        "server_starts": dns_server_starts,
        "server_stops": dns_server_stops,
        "sample_logs": dns_logs[:20]
    }


def analyze_telegram_logs(app_log_file: Path, logcat_file: Path) -> Dict[str, Any]:
    """Telegram loglarƒ±nƒ± analiz et (app_log ve logcat'ten filtrele)."""
    telegram_logs = []
    telegram_errors = 0
    telegram_success = 0
    commands_processed = 0
    notifications_sent = 0
    api_calls = 0
    api_errors = 0

    # Telegram ile ilgili tag'ler
    telegram_tags = ["TelegramNotificationManager",
                     "TelegramApiDataSource", "TelegramConfig", "Telegram"]

    # App log'dan Telegram loglarƒ±nƒ± filtrele
    if app_log_file and app_log_file.exists():
        content = app_log_file.read_text(encoding="utf-8", errors="ignore")
        for line in content.split("\n"):
            if any(tag in line for tag in telegram_tags):
                telegram_logs.append(line)

                # Ba≈üarƒ±lƒ± i≈ülemler
                if "sent successfully" in line.lower() or "successfully" in line.lower():
                    telegram_success += 1
                if "notification sent" in line.lower():
                    notifications_sent += 1
                if "command processed" in line.lower():
                    commands_processed += 1
                if "Sending message" in line or "getUpdates" in line:
                    api_calls += 1

                # Hatalar
                if "ERROR" in line.upper() or "FAILED" in line.upper() or "Exception" in line:
                    telegram_errors += 1
                    api_errors += 1

    # Logcat'ten Telegram loglarƒ±nƒ± filtrele
    if logcat_file and logcat_file.exists():
        content = logcat_file.read_text(encoding="utf-8", errors="ignore")
        for line in content.split("\n"):
            if any(tag in line for tag in telegram_tags):
                telegram_logs.append(line)

                if "sent successfully" in line.lower():
                    telegram_success += 1
                if "notification sent" in line.lower():
                    notifications_sent += 1
                if "command processed" in line.lower():
                    commands_processed += 1
                if "Sending message" in line or "getUpdates" in line:
                    api_calls += 1

                if "ERROR" in line.upper() or "FATAL" in line.upper():
                    telegram_errors += 1
                    api_errors += 1

    if not telegram_logs:
        return {"status": "not_found"}

    success_rate = (telegram_success / (telegram_success + telegram_errors)
                    * 100) if (telegram_success + telegram_errors) > 0 else 0

    return {
        "status": "analyzed",
        "total_telegram_logs": len(telegram_logs),
        "success_count": telegram_success,
        "error_count": telegram_errors,
        "success_rate": round(success_rate, 2),
        "notifications_sent": notifications_sent,
        "commands_processed": commands_processed,
        "api_calls": api_calls,
        "api_errors": api_errors,
        "sample_logs": telegram_logs[:15]
    }


def analyze_logcat(log_file: Path) -> Dict[str, Any]:
    """Logcat loglarƒ±nƒ± analiz et."""
    if not log_file or not log_file.exists():
        return {"status": "not_found"}

    content = log_file.read_text(encoding="utf-8", errors="ignore")
    lines = content.split("\n")

    # Log seviyeleri
    level_counts = Counter()
    tag_counts = Counter()
    fatal_errors = []

    for line in lines:
        if not line.strip():
            continue

        # Android log formatƒ±: MM-DD HH:MM:SS.mmm PID TID LEVEL TAG: MESSAGE
        parts = line.split()
        if len(parts) >= 5:
            level = parts[4] if len(parts) > 4 else "UNKNOWN"
            level_counts[level] += 1

            if len(parts) > 5:
                tag = parts[5].rstrip(":")
                tag_counts[tag] += 1

        # Fatal hatalarƒ± topla
        if "FATAL" in line or "AndroidRuntime" in line and "FATAL" in line:
            fatal_errors.append(line[:300])

    return {
        "status": "analyzed",
        "total_lines": len(lines),
        "level_counts": dict(level_counts),
        "top_tags": dict(tag_counts.most_common(15)),
        "fatal_errors": fatal_errors[:10]
    }


def generate_report(analyses: Dict[str, Any]) -> str:
    """Markdown formatƒ±nda rapor olu≈ütur."""
    report = []

    # Ba≈ülƒ±k
    report.append("# HyperXray Log Analiz Raporu\n")
    report.append(
        f"**Olu≈üturulma Tarihi:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.append(f"**Package:** {PACKAGE_NAME}\n")
    report.append("---\n")

    # √ñzet
    report.append("## üìä √ñzet\n")
    report.append("| Log Tipi | Durum | Detaylar |\n")
    report.append("|----------|-------|----------|\n")

    for log_type, analysis in analyses.items():
        status = analysis.get("status", "unknown")
        status_emoji = "‚úÖ" if status == "analyzed" else "‚ö†Ô∏è" if status == "not_found" else "‚ùå"

        if log_type == "app_log":
            if status == "analyzed":
                total = analysis.get("total_lines", 0)
                errors = analysis.get("error_count", 0)
                report.append(
                    f"| Ana Log | {status_emoji} {status} | {total} satƒ±r, {errors} hata |\n")
            else:
                report.append(f"| Ana Log | {status_emoji} {status} | - |\n")

        elif log_type == "learner_log":
            if status == "analyzed":
                entries = analysis.get("total_entries", 0)
                success = analysis.get("success_rate", 0)
                report.append(
                    f"| √ñƒürenme Logu | {status_emoji} {status} | {entries} kayƒ±t, %{success} ba≈üarƒ± |\n")
            else:
                report.append(
                    f"| √ñƒürenme Logu | {status_emoji} {status} | - |\n")

        elif log_type == "runtime_log":
            if status == "analyzed":
                entries = analysis.get("total_entries", 0)
                success = analysis.get("success_rate", 0)
                report.append(
                    f"| Runtime Logu | {status_emoji} {status} | {entries} kayƒ±t, %{success} ba≈üarƒ± |\n")
            else:
                report.append(
                    f"| Runtime Logu | {status_emoji} {status} | - |\n")

        elif log_type == "dns_cache":
            if status == "analyzed":
                entries = analysis.get("total_entries", 0)
                report.append(
                    f"| DNS Cache | {status_emoji} {status} | {entries} kayƒ±t |\n")
            else:
                report.append(f"| DNS Cache | {status_emoji} {status} | - |\n")

        elif log_type == "dns_logs":
            if status == "analyzed":
                logs = analysis.get("total_dns_logs", 0)
                hit_rate = analysis.get("cache_hit_rate", 0)
                report.append(
                    f"| DNS Loglarƒ± | {status_emoji} {status} | {logs} log, %{hit_rate} cache hit |\n")
            else:
                report.append(
                    f"| DNS Loglarƒ± | {status_emoji} {status} | - |\n")

        elif log_type == "telegram_logs":
            if status == "analyzed":
                logs = analysis.get("total_telegram_logs", 0)
                success_rate = analysis.get("success_rate", 0)
                report.append(
                    f"| Telegram Loglarƒ± | {status_emoji} {status} | {logs} log, %{success_rate} ba≈üarƒ± |\n")
            else:
                report.append(
                    f"| Telegram Loglarƒ± | {status_emoji} {status} | - |\n")

        elif log_type == "stat_file":
            if status == "analyzed":
                total = analysis.get("total_lines", 0)
                report.append(
                    f"| Stat Dosyasƒ± | {status_emoji} {status} | {total} satƒ±r |\n")
            else:
                report.append(f"| Stat Dosyasƒ± | {status_emoji} {status} | - |\n")

        elif log_type == "logcat":
            if status == "analyzed":
                lines = analysis.get("total_lines", 0)
                fatals = len(analysis.get("fatal_errors", []))
                report.append(
                    f"| Logcat | {status_emoji} {status} | {lines} satƒ±r, {fatals} fatal hata |\n")
            else:
                report.append(f"| Logcat | {status_emoji} {status} | - |\n")

    report.append("\n")

    # Detaylƒ± analizler
    for log_type, analysis in analyses.items():
        if analysis.get("status") != "analyzed":
            continue

        report.append(f"## üìã {log_type.replace('_', ' ').title()} Detaylarƒ±\n")

        if log_type == "app_log":
            report.append(
                f"- **Toplam Satƒ±r:** {analysis.get('total_lines', 0):,}\n")

            if analysis.get("time_range"):
                tr = analysis["time_range"]
                report.append(
                    f"- **Zaman Aralƒ±ƒüƒ±:** {tr['first']} - {tr['last']}\n")

            report.append("\n### Log Seviyeleri\n")
            report.append("| Seviye | Sayƒ± |\n")
            report.append("|--------|------|\n")
            for level, count in analysis.get("level_counts", {}).items():
                report.append(f"| {level} | {count:,} |\n")

            report.append("\n### En √áok Kullanƒ±lan Tag'ler\n")
            report.append("| Tag | Kullanƒ±m |\n")
            report.append("|-----|----------|\n")
            for tag, count in list(analysis.get("top_tags", {}).items())[:10]:
                report.append(f"| {tag} | {count:,} |\n")

            if analysis.get("error_count", 0) > 0:
                report.append("\n### √ñrnek Hatalar\n")
                report.append("```\n")
                for error in analysis.get("sample_errors", [])[:5]:
                    report.append(f"{error}\n")
                report.append("```\n")

        elif log_type in ["learner_log", "runtime_log"]:
            report.append(
                f"- **Toplam Kayƒ±t:** {analysis.get('total_entries', 0):,}\n")
            report.append(
                f"- **Ba≈üarƒ± Oranƒ±:** %{analysis.get('success_rate', 0):.2f}\n")
            report.append(
                f"- **Ortalama Gecikme:** {analysis.get('avg_latency_ms', 0):.2f} ms\n")
            report.append(
                f"- **Ortalama Bant Geni≈üliƒüi:** {analysis.get('avg_throughput_kbps', 0):.2f} kbps\n")

            if "route_decisions" in analysis:
                report.append("\n### Rota Kararlarƒ±\n")
                report.append("| Karar | Sayƒ± |\n")
                report.append("|-------|------|\n")
                for decision, count in analysis["route_decisions"].items():
                    decision_name = {0: "Proxy", 1: "Direct", 2: "Optimized"}.get(
                        decision, f"Unknown({decision})")
                    report.append(f"| {decision_name} | {count:,} |\n")

            if "routing_decisions" in analysis:
                report.append("\n### Routing Kararlarƒ±\n")
                report.append("| Karar | Sayƒ± |\n")
                report.append("|-------|------|\n")
                for decision, count in analysis["routing_decisions"].items():
                    report.append(f"| {decision} | {count:,} |\n")

        elif log_type == "dns_cache":
            report.append(
                f"- **Toplam Kayƒ±t:** {analysis.get('total_entries', 0):,}\n")
            report.append(
                f"- **Ge√ßerli Kayƒ±t:** {analysis.get('valid_entries', 0):,}\n")
            report.append(
                f"- **S√ºresi Dolmu≈ü Kayƒ±t:** {analysis.get('expired_entries', 0):,}\n")
            report.append(
                f"- **S√ºresi Dolma Oranƒ±:** %{analysis.get('expiry_rate', 0):.2f}\n")
            report.append(
                f"- **Ortalama TTL:** {analysis.get('avg_ttl_seconds', 0):.2f} saniye ({analysis.get('avg_ttl_seconds', 0) / 3600:.2f} saat)\n")
            report.append(
                f"- **Min TTL:** {analysis.get('min_ttl_seconds', 0):,} saniye\n")
            report.append(
                f"- **Max TTL:** {analysis.get('max_ttl_seconds', 0):,} saniye\n")
            report.append(
                f"- **Ortalama Ya≈ü:** {analysis.get('avg_age_seconds', 0):.2f} saniye ({analysis.get('avg_age_seconds', 0) / 3600:.2f} saat)\n")
            report.append(
                f"- **Entry Ba≈üƒ±na Ortalama IP:** {analysis.get('avg_ips_per_entry', 0):.2f}\n")
            report.append(
                f"- **Cache Boyutu:** {analysis.get('cache_size_bytes', 0):,} bytes ({analysis.get('cache_size_bytes', 0) / 1024:.2f} KB)\n")
            
            if analysis.get("ttl_distribution"):
                report.append("\n### TTL Daƒüƒ±lƒ±mƒ±\n")
                report.append("| Aralƒ±k | Kayƒ±t Sayƒ±sƒ± |\n")
                report.append("|--------|--------------|\n")
                for range_name, count in analysis.get("ttl_distribution", {}).items():
                    report.append(f"| {range_name} | {count:,} |\n")
            
            if analysis.get("domain_categories"):
                report.append("\n### Domain Kategorileri\n")
                report.append("| Kategori | Sayƒ± |\n")
                report.append("|----------|------|\n")
                for category, count in analysis.get("domain_categories", {}).items():
                    report.append(f"| {category} | {count:,} |\n")
            
            report.append(
                f"- **Pop√ºler Domain Sayƒ±sƒ±:** {analysis.get('popular_domains_count', 0)}\n")

            if analysis.get("sample_domains"):
                report.append("\n### √ñrnek Domainler (ƒ∞lk 30)\n")
                report.append("| Domain |\n")
                report.append("|--------|\n")
                for domain in analysis.get("sample_domains", [])[:30]:
                    report.append(f"| {domain} |\n")

        elif log_type == "dns_logs":
            report.append(
                f"- **Toplam DNS Log:** {analysis.get('total_dns_logs', 0):,}\n")
            report.append(
                f"- **Cache Hit:** {analysis.get('cache_hits', 0):,}\n")
            report.append(
                f"- **Cache Miss:** {analysis.get('cache_misses', 0):,}\n")
            report.append(
                f"- **DNS Resolved:** {analysis.get('dns_resolved', 0):,}\n")
            report.append(
                f"- **Cache Hit Rate:** %{analysis.get('cache_hit_rate', 0):.2f}\n")
            report.append(f"- **Hatalar:** {analysis.get('errors', 0):,}\n")
            if analysis.get('unhealthy_servers', 0) > 0:
                report.append(
                    f"- **‚ö†Ô∏è Unhealthy DNS Servers:** {analysis.get('unhealthy_servers', 0):,}\n")
            if analysis.get('invalid_data_errors', 0) > 0:
                report.append(
                    f"- **‚ö†Ô∏è Invalid Data Errors:** {analysis.get('invalid_data_errors', 0):,}\n")
            report.append(
                f"- **Benzersiz Domain:** {analysis.get('unique_domains', 0):,}\n")
            
            if analysis.get("latency_stats"):
                lat_stats = analysis["latency_stats"]
                report.append("\n### Latency ƒ∞statistikleri\n")
                report.append(f"- **Ortalama Latency:** {lat_stats.get('avg_ms', 0):.2f} ms\n")
                report.append(f"- **Min Latency:** {lat_stats.get('min_ms', 0):.2f} ms\n")
                report.append(f"- **Max Latency:** {lat_stats.get('max_ms', 0):.2f} ms\n")
                report.append(f"- **√ñrnek Sayƒ±sƒ±:** {lat_stats.get('samples', 0):,}\n")
            
            if analysis.get("ttl_stats"):
                ttl_stats = analysis["ttl_stats"]
                report.append("\n### TTL ƒ∞statistikleri (Loglardan)\n")
                report.append(f"- **Ortalama TTL:** {ttl_stats.get('avg_seconds', 0):.2f} saniye ({ttl_stats.get('avg_seconds', 0) / 3600:.2f} saat)\n")
                report.append(f"- **Min TTL:** {ttl_stats.get('min_seconds', 0):,} saniye\n")
                report.append(f"- **Max TTL:** {ttl_stats.get('max_seconds', 0):,} saniye\n")
                report.append(f"- **√ñrnek Sayƒ±sƒ±:** {ttl_stats.get('samples', 0):,}\n")
            
            if analysis.get("operations"):
                report.append("\n### DNS ƒ∞≈ülemleri\n")
                report.append("| ƒ∞≈ülem | Sayƒ± |\n")
                report.append("|-------|------|\n")
                for op, count in analysis.get("operations", {}).items():
                    report.append(f"| {op} | {count:,} |\n")
            
            if analysis.get("metrics_updates", 0) > 0:
                report.append(f"- **Metrics G√ºncellemeleri:** {analysis.get('metrics_updates', 0):,}\n")
            if analysis.get("server_starts", 0) > 0:
                report.append(f"- **DNS Server Ba≈ülatmalarƒ±:** {analysis.get('server_starts', 0):,}\n")
            if analysis.get("server_stops", 0) > 0:
                report.append(f"- **DNS Server Durdurmalarƒ±:** {analysis.get('server_stops', 0):,}\n")

            if analysis.get("top_domains"):
                report.append("\n### En √áok Sorgulanan Domainler (ƒ∞lk 30)\n")
                report.append("| Domain | Sorgu Sayƒ±sƒ± |\n")
                report.append("|--------|--------------|\n")
                for domain, count in list(analysis.get("top_domains", {}).items())[:30]:
                    report.append(f"| {domain} | {count:,} |\n")

            if analysis.get("sample_logs"):
                report.append("\n### √ñrnek DNS Loglarƒ±\n")
                report.append("```\n")
                for log in analysis.get("sample_logs", [])[:5]:
                    report.append(f"{log[:200]}\n")
                report.append("```\n")

        elif log_type == "telegram_logs":
            report.append(
                f"- **Toplam Telegram Log:** {analysis.get('total_telegram_logs', 0):,}\n")
            report.append(
                f"- **Ba≈üarƒ±lƒ± ƒ∞≈ülemler:** {analysis.get('success_count', 0):,}\n")
            report.append(
                f"- **Hatalar:** {analysis.get('error_count', 0):,}\n")
            report.append(
                f"- **Ba≈üarƒ± Oranƒ±:** %{analysis.get('success_rate', 0):.2f}\n")
            report.append(
                f"- **G√∂nderilen Bildirimler:** {analysis.get('notifications_sent', 0):,}\n")
            report.append(
                f"- **ƒ∞≈ülenen Komutlar:** {analysis.get('commands_processed', 0):,}\n")
            report.append(
                f"- **API √áaƒürƒ±larƒ±:** {analysis.get('api_calls', 0):,}\n")
            report.append(
                f"- **API Hatalarƒ±:** {analysis.get('api_errors', 0):,}\n")

            if analysis.get("sample_logs"):
                report.append("\n### √ñrnek Telegram Loglarƒ±\n")
                report.append("```\n")
                for log in analysis.get("sample_logs", [])[:10]:
                    report.append(f"{log[:200]}\n")
                report.append("```\n")

        elif log_type == "logcat":
            report.append(
                f"- **Toplam Satƒ±r:** {analysis.get('total_lines', 0):,}\n")

            report.append("\n### Log Seviyeleri\n")
            report.append("| Seviye | Sayƒ± |\n")
            report.append("|--------|------|\n")
            for level, count in analysis.get("level_counts", {}).items():
                report.append(f"| {level} | {count:,} |\n")

            report.append("\n### En √áok Kullanƒ±lan Tag'ler\n")
            report.append("| Tag | Kullanƒ±m |\n")
            report.append("|-----|----------|\n")
            for tag, count in list(analysis.get("top_tags", {}).items())[:15]:
                report.append(f"| {tag} | {count:,} |\n")

            if analysis.get("fatal_errors"):
                report.append("\n### Fatal Hatalar\n")
                report.append("```\n")
                for error in analysis.get("fatal_errors", [])[:5]:
                    report.append(f"{error}\n")
                report.append("```\n")

        report.append("\n")

    # Sonu√ß ve √ñneriler
    report.append("## üí° Sonu√ß ve √ñneriler\n")

    total_errors = sum(
        a.get("error_count", 0) for a in analyses.values()
        if isinstance(a, dict) and "error_count" in a
    )

    if total_errors > 0:
        report.append(
            f"‚ö†Ô∏è  **{total_errors} hata tespit edildi.** Detaylƒ± inceleme √∂nerilir.\n")
    else:
        report.append("‚úÖ **Kritik hata tespit edilmedi.**\n")

    report.append("\n### √ñneriler:\n")
    report.append("1. Log dosyalarƒ±nƒ± d√ºzenli olarak temizleyin\n")
    report.append("2. Hata loglarƒ±nƒ± d√ºzenli olarak inceleyin\n")
    report.append("3. Performance metriklerini takip edin\n")
    report.append("4. Log rotation ayarlarƒ±nƒ± kontrol edin\n")

    return "".join(report)


def main():
    """Ana fonksiyon."""
    print("=" * 60)
    print("HyperXray Log Collector ve Rapor Olu≈üturucu")
    print("=" * 60)
    print()

    # ADB baƒülantƒ±sƒ±nƒ± kontrol et
    if not check_adb_connection():
        print(
            "\n‚ùå ADB baƒülantƒ±sƒ± kurulamadƒ±. L√ºtfen cihazƒ±nƒ±zƒ±n baƒülƒ± olduƒüundan emin olun.")
        sys.exit(1)

    print("\nüì• Log dosyalarƒ± √ßekiliyor...\n")

    # Log dosyalarƒ±nƒ± √ßek
    log_files = {}
    log_files["app_log"] = pull_log_file(
        DEVICE_LOG_PATHS["app_log"],
        "app_log.txt",
        check_exists=False  # Bo≈ü olsa bile √ßek
    )
    log_files["learner_log"] = pull_log_file(
        DEVICE_LOG_PATHS["learner_log"],
        "learner_log.jsonl",
        check_exists=False
    )
    log_files["runtime_log"] = pull_log_file(
        DEVICE_LOG_PATHS["runtime_log"],
        "tls_v5_runtime_log.jsonl",
        check_exists=False
    )
    # DNS cache dosyasƒ±nƒ± √ßek (cache dizini i√ßin √∂zel i≈ülem)
    dns_cache_result = run_adb_command(
        ["shell", "run-as", PACKAGE_NAME, "cat", "cache/dns_cache.json"])
    if dns_cache_result:
        dns_cache_local = OUTPUT_DIR / "dns_cache.json"
        dns_cache_local.write_text(
            dns_cache_result, encoding="utf-8", errors="ignore")
        size = dns_cache_local.stat().st_size
        print(f"‚úÖ dns_cache.json √ßekildi ({size:,} bytes)")
        log_files["dns_cache"] = dns_cache_local
    else:
        print(f"‚ö†Ô∏è  DNS cache dosyasƒ± bulunamadƒ±: cache/dns_cache.json")
        log_files["dns_cache"] = None

    # Stat dosyasƒ±nƒ± √ßek
    log_files["stat_file"] = pull_log_file(
        DEVICE_LOG_PATHS["stat_file"],
        "stat.txt",
        check_exists=False
    )

    # Crash dosyalarƒ±nƒ± √ßek
    print("\nüìÇ Crash dosyalarƒ± kontrol ediliyor...")
    crashes_list = run_adb_command(
        ["shell", "run-as", PACKAGE_NAME, "ls", "files/crashes/"])
    if crashes_list:
        crash_files = [f.strip() for f in crashes_list.split("\n") if f.strip()]
        if crash_files:
            print(f"  {len(crash_files)} crash dosyasƒ± bulundu")
            for crash_file in crash_files[:5]:  # ƒ∞lk 5 crash dosyasƒ±nƒ± √ßek
                crash_result = run_adb_command(
                    ["shell", "run-as", PACKAGE_NAME, "cat", f"files/crashes/{crash_file}"])
                if crash_result:
                    crash_local = OUTPUT_DIR / f"crash_{crash_file}"
                    crash_local.write_text(crash_result, encoding="utf-8", errors="ignore")
                    size = crash_local.stat().st_size
                    print(f"  ‚úÖ crash_{crash_file} √ßekildi ({size:,} bytes)")

    # T√ºm dosyalarƒ± listele (debug i√ßin)
    print("\nüìÇ Uygulama dosyalarƒ± listeleniyor...")
    files_list = run_adb_command(
        ["shell", "run-as", PACKAGE_NAME, "ls", "-la", "files/"])
    if files_list:
        print("Files dizini i√ßeriƒüi:")
        for line in files_list.split("\n")[:20]:  # ƒ∞lk 20 satƒ±r
            if line.strip():
                print(f"  {line}")
    
    # Logs dizinini kontrol et ve t√ºm dosyalarƒ± √ßek
    print("\nüìÇ Logs dizini kontrol ediliyor...")
    logs_list = run_adb_command(
        ["shell", "run-as", PACKAGE_NAME, "ls", "-la", "files/logs/"])
    if logs_list:
        print("Logs dizini i√ßeriƒüi:")
        log_files_in_logs = []
        for line in logs_list.split("\n"):
            if line.strip() and not line.startswith("total") and not line.startswith("d"):
                # Dosya adƒ±nƒ± √ßƒ±kar
                parts = line.split()
                if len(parts) >= 9:
                    filename = parts[-1]
                    if filename and filename != "." and filename != "..":
                        log_files_in_logs.append(filename)
                        print(f"  {line}")
        
        # Logs dizinindeki t√ºm dosyalarƒ± √ßek
        for log_file_name in log_files_in_logs:
            log_result = run_adb_command(
                ["shell", "run-as", PACKAGE_NAME, "cat", f"files/logs/{log_file_name}"])
            if log_result:
                log_local = OUTPUT_DIR / f"logs_{log_file_name}"
                log_local.write_text(log_result, encoding="utf-8", errors="ignore")
                size = log_local.stat().st_size
                print(f"  ‚úÖ logs_{log_file_name} √ßekildi ({size:,} bytes)")

    log_files["logcat"] = collect_logcat()

    print("\nüìä Loglar analiz ediliyor...\n")

    # Loglarƒ± analiz et
    analyses = {}
    analyses["app_log"] = analyze_app_log(log_files.get("app_log"))
    analyses["learner_log"] = analyze_jsonl_log(log_files.get("learner_log"))
    analyses["runtime_log"] = analyze_jsonl_log(log_files.get("runtime_log"))
    analyses["dns_cache"] = analyze_dns_cache(log_files.get("dns_cache"))
    analyses["stat_file"] = analyze_app_log(log_files.get("stat_file"))  # Stat dosyasƒ±nƒ± da analiz et
    analyses["dns_logs"] = analyze_dns_logs(
        log_files.get("app_log"), log_files.get("logcat"))
    analyses["telegram_logs"] = analyze_telegram_logs(
        log_files.get("app_log"), log_files.get("logcat"))
    analyses["logcat"] = analyze_logcat(log_files.get("logcat"))

    print("\nüìù Rapor olu≈üturuluyor...\n")

    # Rapor olu≈ütur
    report = generate_report(analyses)
    REPORT_FILE.write_text(report, encoding="utf-8")

    print(f"‚úÖ Rapor olu≈üturuldu: {REPORT_FILE}")
    print(f"\nüìÅ T√ºm log dosyalarƒ±: {OUTPUT_DIR.absolute()}")
    print("\n" + "=" * 60)
    print("Tamamlandƒ±! ‚úÖ")
    print("=" * 60)


if __name__ == "__main__":
    main()
